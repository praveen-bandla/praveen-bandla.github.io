---
layout: research
icon: fa-solid fa-building-columns
order: 4
---

I am working as a Research Assistant to Dr. Rishi Sonthalia to study *the Effect of Geometry and Model Architecture on the Performance of a Graph Neural Network*. Our preliminary work has been accepted to the [SoCal NLP Symposium 2023](https://socalnlp.github.io/symp23/index.html){:target="_blank"} as a poster presentation. We are currently working towards a full paper.

Co-authors: [Xinyue Cui](https://www.linkedin.com/in/xinyue-cui-118b5424a){:target="_blank"}, [Rishi Sonthalia](https://sites.google.com/umich.edu/rsonthal/publications?authuser=0){:target="_blank"}

[[Preprint] ](https://drive.google.com/file/d/14-aPA1EuiABLjA9CnT_qGa_2ZiIqzWL7/view?usp=sharing){:target="_blank"}
[[Code]](https://github.com/praveen-bandla/MixedGeometry){:target="_blank"}


## Background

A Graph Neural Network (GNNs) is a type of neural network specifically designed to capture the dependency patterns in data that is structured as a graph. They are powerful tools for tasks that involve relational data, such as social network analysis, molecular chemistry, and recommendation systems. GNNs exploit the graph structure to infer new information about nodes and edges, providing insights that are not apparent from non-relational data models.

GNNs differ primarily in how they generate node embeddings and their message-passing mechanisms. Popular GNNs like Graph Convolutional Networks ([GCNs](https://arxiv.org/abs/1609.02907){:target="_blank"}) and Graph Attention Networks ([GATs](https://arxiv.org/abs/1710.10903){:target="_blank"}) use different strategies for aggregating information from a node’s neighbors to update its representation, which greatly influences their performance across various tasks.

The concept of a hyperbolic GNN extends the idea of neural networks to hyperbolic space — a non-Euclidean geometry that is better suited for capturing hierarchical structures. Studies have shown that hyperbolic GNNs can be particularly effective for data with inherent hierarchical properties, offering lower distortion in data representation and improved performance ([Chami et al., 2019](https://arxiv.org/abs/1910.12892){:target="_blank"}; [Ganea et al., 2018](https://neurips.cc/Conferences/2018/ScheduleMultitrack?event=10927){:target="_blank"}).


## Our Work

Our research is centered on the study of how the choice of geometry and the architecture of Graph Neural Networks (GNNs) influence their learning efficacy and performance. We are conducting experiments with different combinations of model configurations across datasets with varying properties to assess their relative performance. The goal of our work is to provide insights into the selection of GNN architecture and geometry space using graph characteristics.

Stay tuned for updates on our research!




